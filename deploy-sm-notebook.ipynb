{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.mxnet.model import MXNetModel\n",
    "from sagemaker import get_execution_role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "model_data = 's3://<your Amazon S3 bucket name>/gpt2-model/model.tar.gz'\n",
    "entry_point = './gpt2-inference.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To define MXNetModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mxnet_model = MXNetModel(model_data=model_data,\n",
    "                         role=role,\n",
    "                         entry_point=entry_point,\n",
    "                         py_version='py3',\n",
    "                         framework_version='1.6.0',\n",
    "                         image='<image uri of the container image>,\n",
    "                         model_server_workers=2\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy model endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = mxnet_model.deploy(instance_type='ml.c5.large', initial_instance_count=1)\n",
    "print(predictor.endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run a simple performance test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.mxnet.model import MXNetPredictor\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "endpoint_name = '<ENDPOINT 이름>'\n",
    "predictor = MXNetPredictor(endpoint_name, sagemaker_session)\n",
    "\n",
    "input_sentence = '아기 공룡 둘리는 희동이와'\n",
    "\n",
    "pred_latency_sum = 0\n",
    "pred_count_sum = 0\n",
    "pred_cnt = 0\n",
    "\n",
    "for i in range(20):\n",
    "  try:\n",
    "    pred_out = predictor.predict(input_sentence)\n",
    "    if i == 0:\n",
    "      continue\n",
    "    \n",
    "    predicted_sentence= pred_out[0]\n",
    "    predict_count = pred_out[1]\n",
    "    predict_latency = pred_out[2]\n",
    "  \n",
    "    pred_latency_sum += predict_latency\n",
    "    pred_count_sum =+ predict_count\n",
    "    pred_cnt += 1\n",
    "  except:\n",
    "    print('Error and ingore it.')\n",
    "\n",
    "avg_latency = pred_latency_sum / pred_cnt\n",
    "avg_latency_per_inf = avg_latency / pred_count_sum\n",
    "\n",
    "print('Input sentence: {}'.format(input_sentence))\n",
    "print('Predicted sentence: {}'.format(predicted_sentence))\n",
    "print('Average number of inferenced token: {:.2f}'.format(pred_count_sum))\n",
    "print('Average inference latency for a sentence completion: {:.2f}'.format(avg_latency))\n",
    "print('Average inference latency per a token: {:.2f}\\n'.format(avg_latency_per_inf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean UP!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()\n",
    "predictor.delete_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
